"""
app.py
ê±´ì„¤ë²•ë ¹ ì±—ë´‡ Streamlit
"""

import streamlit as st
import os
from dotenv import load_dotenv
from s4_EmbeddingManager import EmbeddingManager
from s5_LegalSearchEngine import LegalSearchEngine
from enhanced_legal_qa_system import EnhancedLegalQASystem
import json

load_dotenv()

st.set_page_config(
    page_title="ê±´ì„¤ë²•ë ¹ ì±—ë´‡",
    page_icon="ğŸ—ï¸",
    layout="wide"
)

# ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-title {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .query-badge {
        display: inline-block;
        padding: 0.3rem 0.8rem;
        border-radius: 15px;
        font-size: 0.9rem;
        font-weight: bold;
        background-color: #4ecdc4;
        color: white;
    }
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def load_system():
    """ì‹œìŠ¤í…œ ë¡œë“œ"""
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    if not OPENAI_API_KEY:
        st.error("âš ï¸ OPENAI_API_KEY í•„ìš”")
        st.stop()
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    
    vector_store_dir = os.path.join(project_root, "data", "vector_store", "construction_law")
    cache_dir = os.path.join(project_root, "data", "cache")
    
    with st.spinner("ğŸ”§ ì‹œìŠ¤í…œ ë¡œë”©..."):
        em = EmbeddingManager(OPENAI_API_KEY, "construction_law", cache_dir=cache_dir)
        
        index = em.load_index(os.path.join(vector_store_dir, "faiss_index.bin"))
        metadata = em.load_metadata(os.path.join(vector_store_dir, "metadata.json"))
        
        if not index or not metadata:
            st.error("âš ï¸ ì¸ë±ìŠ¤ íŒŒì¼ ì—†ìŒ")
            st.stop()
        
        engine = LegalSearchEngine(index, metadata, em)
        qa_system = EnhancedLegalQASystem(engine, OPENAI_API_KEY)
    
    return qa_system


# ë©”ì¸
st.markdown('<p class="main-title">ğŸ—ï¸ ê±´ì„¤ë²•ë ¹ AI ì±—ë´‡</p>', unsafe_allow_html=True)

qa_system = load_system()

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“– ì‚¬ìš© ê°€ì´ë“œ")
    st.markdown("""
    **ì§ˆë¬¸ ìœ í˜•:**
    - ğŸ”´ ë²•ì¡°ë¬¸: "ì œ36ì¡° ë‚´ìš©"
    - ğŸŸ¢ ì •ë³´: "ë¹„ê³„ ì•ˆì „ ê¸°ì¤€"
    - ğŸ”µ ì»¨ì„¤íŒ…: "3m ë¹„ê³„ ê´œì°®ì•„?"
    - ğŸŸ¡ ì ˆì°¨: "ìš©ë„ë³€ê²½ ì ˆì°¨"
    - ğŸŸ  ë¬¸ì„œ: "ì²´í¬ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ì–´"
    - ğŸŸ£ ë¹„êµ: "Aë²•ê³¼ Bë²• ì°¨ì´"
    """)
    
    st.markdown("---")
    show_details = st.checkbox("ìƒì„¸ ì •ë³´ í‘œì‹œ", value=True)

# ì±„íŒ…
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë¹„ê³„ ì•ˆì „ ê¸°ì¤€ì€?)"):
    
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
            
            # QA ì‹¤í–‰
            answer = qa_system.generate_response(prompt, verbose=False)
            
            meta = answer.get("_meta", {})
            classification = meta.get("classification", {})
            
            # ìœ í˜• í‘œì‹œ
            query_type = classification.get("query_type", "ì¼ë°˜_ì •ë³´_ê²€ìƒ‰")
            confidence = classification.get("confidence", 0)
            
            st.markdown(f"""
            <span class="query-badge">{query_type}</span>
            <span style="color: gray;"> (í™•ì‹ ë„: {confidence:.0%})</span>
            """, unsafe_allow_html=True)
            
            # ë‹µë³€ í‘œì‹œ
            st.markdown("---")
            
            # ìœ í˜•ë³„ ë Œë”ë§
            if query_type == "ë²•ì¡°ë¬¸_ì¡°íšŒ":
                ë²•ì¡°ë¬¸ = answer.get("ë²•ì¡°ë¬¸", {})
                st.markdown(f"### ğŸ“œ {ë²•ì¡°ë¬¸.get('ë²•ë ¹ëª…', 'N/A')} {ë²•ì¡°ë¬¸.get('ì¡°í•­', 'N/A')}")
                st.info(ë²•ì¡°ë¬¸.get("ì¡°ë¬¸_ë‚´ìš©", ""))
                if ë²•ì¡°ë¬¸.get("ê°„ë‹¨_í•´ì„¤"):
                    st.write("**í•´ì„¤:**", ë²•ì¡°ë¬¸["ê°„ë‹¨_í•´ì„¤"])
            
            elif query_type == "ì¼ë°˜_ì •ë³´_ê²€ìƒ‰":
                ì£¼ì œ = answer.get("ì£¼ì œ", "")
                ë²•ì _ê·¼ê±° = answer.get("ë²•ì _ê·¼ê±°", {})
                
                if ì£¼ì œ:
                    st.markdown(f"### ğŸ“‹ {ì£¼ì œ}")
                
                if ë²•ì _ê·¼ê±°.get("í•µì‹¬_ìš”êµ¬ì‚¬í•­"):
                    st.write("**í•µì‹¬ ìš”êµ¬ì‚¬í•­:**", ë²•ì _ê·¼ê±°["í•µì‹¬_ìš”êµ¬ì‚¬í•­"])
                
                if ë²•ì _ê·¼ê±°.get("ì¤€ìˆ˜_ë°©ë²•"):
                    st.write("**ì¤€ìˆ˜ ë°©ë²•:**")
                    for m in ë²•ì _ê·¼ê±°["ì¤€ìˆ˜_ë°©ë²•"]:
                        st.write(f"- {m}")
            
            elif query_type == "ìƒí™©ë³„_ì»¨ì„¤íŒ…":
                ë²•ì _íŒë‹¨ = answer.get("ë²•ì _íŒë‹¨", {})
                ê²°ë¡  = ë²•ì _íŒë‹¨.get("ê²°ë¡ ", "")
                
                if "ì ë²•" in ê²°ë¡ :
                    st.success(f"**ê²°ë¡ :** {ê²°ë¡ }")
                elif "ë¶€ì ë²•" in ê²°ë¡ :
                    st.error(f"**ê²°ë¡ :** {ê²°ë¡ }")
                else:
                    st.warning(f"**ê²°ë¡ :** {ê²°ë¡ }")
                
                if ë²•ì _íŒë‹¨.get("ê·¼ê±°"):
                    st.write("**ê·¼ê±°:**", ë²•ì _íŒë‹¨["ê·¼ê±°"])
            
            else:
                # ê¸°ë³¸ JSON í‘œì‹œ
                st.json(answer)
            
            # ìƒì„¸ ì •ë³´
            if show_details:
                with st.expander("ğŸ“š ì°¸ì¡° ë¬¸ì„œ"):
                    for i, s in enumerate(meta.get("sources", []), 1):
                        st.write(f"**[{i}]** {s['doc_name']} (p.{s['page']}) - ê´€ë ¨ë„: {s['relevance_score']:.3f}")
            
            # ì €ì¥
            response_text = json.dumps(answer, ensure_ascii=False)
            st.session_state.messages.append({"role": "assistant", "content": response_text})