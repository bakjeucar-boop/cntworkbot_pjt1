"""
s3_LegalChunkingStrategy.py
ë²•ë ¹ ë¬¸ì„œ ì²­í‚¹

ë©”íƒ€ë°ì´í„° êµ¬ì¡°:
- doc_id, doc_name, page: ë¬¸ì„œ ì‹ë³„
- chunk_tokens: í† í° ìˆ˜
- has_overlap: ì˜¤ë²„ë© ì—¬ë¶€
"""

import re
import json
import os
from typing import List, Dict
import tiktoken


class LegalChunkingStrategy:
    """ë²•ë ¹ ì²­í‚¹ ì „ëµ"""
    
    def __init__(self, chunk_size: int = 800, overlap: int = 200, model: str = "gpt-4"):
        """
        Args:
            chunk_size: ì²­í¬ ìµœëŒ€ í† í° ìˆ˜
            overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í† í° ìˆ˜
            model: í† í° ê³„ì‚°ìš© ëª¨ë¸ëª…
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.encoder = tiktoken.encoding_for_model(model)

        print(f"ğŸ“ ê°„ì†Œí™” ì²­í‚¹ ì „ëµ ì´ˆê¸°í™”")        
        print(f"  - ì²­í¬ í¬ê¸°: {chunk_size} í† í°")
        print(f"  - ì˜¤ë²„ë©: {overlap} í† í°")
        print(f"  - ë©”íƒ€ë°ì´í„°: ìµœì†Œí™” (doc ì •ë³´ë§Œ)")
    
    def count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
        return len(self.encoder.encode(text))
    
    def split_by_tokens(self, text: str, max_tokens: int) -> List[Dict]:
        """
        í† í° ë‹¨ìœ„ë¡œ ê°•ì œ ë¶„í•  (ìµœí›„ì˜ ìˆ˜ë‹¨)
        800í† í°ì”© ì˜ë¼ì„œ ë°˜í™˜
        """
        tokens = self.encoder.encode(text)
        chunks = []
        
        for i in range(0, len(tokens), max_tokens):
            chunk_tokens = tokens[i:i + max_tokens]
            chunk_text = self.encoder.decode(chunk_tokens)
            chunks.append({"text": chunk_text})
        
        return chunks
    
    def split_by_article(self, text: str, max_tokens: int) -> List[Dict]:
        """
        ì¡°(æ¢) ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
        
        ë¶„í•  ìš°ì„ ìˆœìœ„:
        1. ì¡°(æ¢) ë‹¨ìœ„
        2. í•­(é …) ë‹¨ìœ„  
        3. í† í° ì œí•œ
        """
        # ì œâ—‹ì¡° íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        parts = re.split(r'(ì œ\d+ì¡°(?:ì˜\d+)?)', text)
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        i = 0
        while i < len(parts):
            part = parts[i].strip()
            
            if not part:
                i += 1
                continue
            
            # ì œâ—‹ì¡° íŒ¨í„´
            if re.match(r'ì œ\d+ì¡°(?:ì˜\d+)?$', part):
                article = part
                content = ""
                
                if i + 1 < len(parts):
                    content = parts[i + 1].strip()
                
                full_text = article + " " + content
                text_tokens = self.count_tokens(full_text)
                
                # ì¡° ì „ì²´ê°€ max_tokens ì´í•˜
                if text_tokens <= max_tokens:
                    if current_tokens + text_tokens <= max_tokens:
                        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
                        current_chunk += (" " if current_chunk else "") + full_text
                        current_tokens += text_tokens
                    else:
                        # í˜„ì¬ ì²­í¬ ì €ì¥
                        if current_chunk:
                            chunks.append({"text": current_chunk})
                        # ìƒˆ ì²­í¬ ì‹œì‘
                        current_chunk = full_text
                        current_tokens = text_tokens
                else:
                    # ì¡°ê°€ ë„ˆë¬´ í¬ë©´ í•­ ë‹¨ìœ„ë¡œ ë¶„í• 
                    if current_chunk:
                        chunks.append({"text": current_chunk})
                    
                    para_chunks = self._split_by_paragraph(article, content, max_tokens)
                    chunks.extend(para_chunks)
                    
                    current_chunk = ""
                    current_tokens = 0
                
                i += 2
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸
                part_tokens = self.count_tokens(part)
                
                if part_tokens > max_tokens:
                    # í˜„ì¬ ì²­í¬ ì €ì¥
                    if current_chunk:
                        chunks.append({"text": current_chunk})
                    
                    # í† í° ë‹¨ìœ„ë¡œ ê°•ì œ ë¶„í• 
                    token_chunks = self.split_by_tokens(part, max_tokens)
                    chunks.extend(token_chunks)
                    
                    # ìƒˆ ì²­í¬ ì‹œì‘
                    current_chunk = ""
                    current_tokens = 0
                elif current_tokens + part_tokens <= max_tokens:
                    current_chunk += (" " if current_chunk else "") + part
                    current_tokens += part_tokens
                else:
                    if current_chunk:
                        chunks.append({"text": current_chunk})
                    current_chunk = part
                    current_tokens = part_tokens
                
                i += 1
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunks.append({"text": current_chunk})
        
        return chunks
    
    def _split_by_paragraph(self, article: str, content: str, max_tokens: int) -> List[Dict]:
        """í•­ ë‹¨ìœ„ë¡œ ë¶„í• """
        # â‘ â‘¡â‘¢ ë˜ëŠ” 1. 2. 3. íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        parts = re.split(r'([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]|\d+\.)', content)
        
        chunks = []
        current_chunk = article + " "
        current_tokens = self.count_tokens(current_chunk)
        
        for part in parts:
            if not part.strip():
                continue
            
            part_tokens = self.count_tokens(part)
            
            if part_tokens > max_tokens:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk.strip() != article:
                    chunks.append({"text": current_chunk.strip()})
                
                # í•­ì„ í† í° ë‹¨ìœ„ë¡œ ê°•ì œ ë¶„í• 
                token_chunks = self.split_by_tokens(article + " " + part, max_tokens)
                chunks.extend(token_chunks)
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = article + " "
                current_tokens = self.count_tokens(current_chunk)
            elif current_tokens + part_tokens <= max_tokens:
                current_chunk += part + " "
                current_tokens += part_tokens
            else:
                if current_chunk.strip() != article:
                    chunks.append({"text": current_chunk.strip()})
                current_chunk = article + " " + part + " "
                current_tokens = self.count_tokens(current_chunk)
        
        if current_chunk.strip() != article:
            chunks.append({"text": current_chunk.strip()})
        
        return chunks
    
    def process_from_unified_json(self, json_path: str) -> List[Dict]:
        """
        í†µí•© JSONì—ì„œ ì²­í‚¹ ìˆ˜í–‰
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ with ê°„ì†Œí™”ëœ ë©”íƒ€ë°ì´í„°
        """
        print(f"\nğŸ“– JSON ë¡œë“œ: {json_path}")
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        text_blocks = data.get('text_blocks', [])
        print(f"âœ“ {len(text_blocks)}ê°œ í…ìŠ¤íŠ¸ ë¸”ë¡")
        
        all_chunks = []
        chunk_counter = 1
        
        print(f"\nğŸ”ª ì²­í‚¹ ì‹œì‘...")
        
        for idx, block in enumerate(text_blocks, 1):
            text = block['text']
            
            # êµ¬ì¡° ì¸ì‹ ì²­í‚¹
            text_chunks = self.split_by_article(text, self.chunk_size)
            
            for chunk_data in text_chunks:
                chunk_text = chunk_data["text"]
                
                # ê°„ì†Œí™”ëœ ë©”íƒ€ë°ì´í„°
                chunk = {
                    "chunk_id": f"chunk_{chunk_counter:05d}",
                    "content": chunk_text,
                    "metadata": {
                        "doc_id": block["doc_id"],
                        "doc_name": block["doc_name"],
                        "page": block.get("page", 0),
                        "chunk_tokens": self.count_tokens(chunk_text)
                    }
                }
                all_chunks.append(chunk)
                chunk_counter += 1
            
            if idx % 50 == 0:
                print(f"  ì§„í–‰: {idx}/{len(text_blocks)} ë¸”ë¡...")
        
        # ì˜¤ë²„ë© ì ìš©
        print(f"\nğŸ”— ì˜¤ë²„ë© ì ìš©...")
        final_chunks = self.apply_overlap(all_chunks)
        
        print(f"âœ… ìµœì¢… {len(final_chunks)}ê°œ ì²­í¬\n")
        
        return final_chunks
    
    def apply_overlap(self, chunks: List[Dict]) -> List[Dict]:
        """ì²­í¬ ê°„ ì˜¤ë²„ë© ì ìš©"""
        if not chunks or self.overlap == 0:
            return chunks
        
        overlapped_chunks = []
        
        for i, chunk in enumerate(chunks):
            content = chunk["content"]
            
            # ë‹¤ìŒ ì²­í¬ê°€ ìˆê³ , ê°™ì€ ë¬¸ì„œì¸ ê²½ìš°
            if i < len(chunks) - 1:
                next_chunk = chunks[i + 1]
                
                if next_chunk["metadata"]["doc_id"] == chunk["metadata"]["doc_id"]:
                    next_content = next_chunk["content"]
                    
                    # ì˜¤ë²„ë© í† í° ì¶”ì¶œ
                    tokens = self.encoder.encode(next_content)
                    if len(tokens) > self.overlap:
                        overlap_tokens = tokens[:self.overlap]
                        overlap_text = self.encoder.decode(overlap_tokens)
                        content += f"\n\n[ë‹¤ìŒ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°]\n{overlap_text}..."
            
            # ì˜¤ë²„ë© ì ìš©ëœ ì²­í¬
            overlapped_chunks.append({
                **chunk,
                "content": content,
                "metadata": {
                    **chunk["metadata"],
                    "has_overlap": i < len(chunks) - 1,
                    "chunk_tokens": self.count_tokens(content)
                }
            })
        
        return overlapped_chunks
    
    def save_chunks(self, chunks: List[Dict], output_path: str):
        """ì²­í¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ì²­í¬ ì €ì¥: {output_path}")
        print(f"  - ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")


def main():
    """ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    print("="*80)
    print("ğŸ”ª ê°„ì†Œí™” ë²•ë ¹ ì²­í‚¹")
    print("="*80)
    
    # ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    
    INPUT_PATH = os.path.join(project_root, "data", "processed", "construction_law_unified.json")
    OUTPUT_PATH = os.path.join(project_root, "data", "chunks", "construction_law_chunks.json")
    
    print(f"\nì…ë ¥: {INPUT_PATH}")
    print(f"ì¶œë ¥: {OUTPUT_PATH}")
    
    if not os.path.exists(INPUT_PATH):
        print(f"\nâœ— ì…ë ¥ íŒŒì¼ ì—†ìŒ: {INPUT_PATH}")
        return
    
    if os.path.exists(OUTPUT_PATH):
        response = input(f"\nâš  íŒŒì¼ ì¡´ì¬. ë®ì–´ì“°ê¸°? (y/n): ")
        if response.lower() != 'y':
            print("ì·¨ì†Œ")
            return
    
    try:
        chunker = LegalChunkingStrategy(
            chunk_size=800,
            overlap=200,
            model="gpt-4"
        )
        
        chunks = chunker.process_from_unified_json(INPUT_PATH)
        chunker.save_chunks(chunks, OUTPUT_PATH)
        
        print("="*80)
        print("âœ… ì²­í‚¹ ì™„ë£Œ!")
        print("="*80)
        
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()