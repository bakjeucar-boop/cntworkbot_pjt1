"""
document_merger.py
ì—¬ëŸ¬ ê°œì˜ processed.json íŒŒì¼ì„ í•˜ë‚˜ë¡œ í†µí•©
"""

import os
import json
from typing import Dict

class DocumentMerger:
    """ì—¬ëŸ¬ processed.json íŒŒì¼ì„ í•˜ë‚˜ë¡œ í†µí•©"""
    
    def __init__(self, processed_dir: str):
        """
        Args:
            processed_dir: processed.json íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        """
        self.processed_dir = processed_dir
        self.documents = []
    
    def load_all_documents(self):
        """ëª¨ë“  processed.json íŒŒì¼ ë¡œë“œ"""
        print(f"\nğŸ“‚ ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {self.processed_dir}")
        
        # processed.jsonìœ¼ë¡œ ëë‚˜ëŠ” íŒŒì¼ë§Œ ì°¾ê¸°
        json_files = [
            f for f in os.listdir(self.processed_dir) 
            if f.endswith('_processed.json')
        ]
        
        print(f"  ë°œê²¬ëœ íŒŒì¼: {len(json_files)}ê°œ")
        
        for i, json_file in enumerate(sorted(json_files), 1):
            file_path = os.path.join(self.processed_dir, json_file)
            
            print(f"\n  [{i}] ë¡œë”©: {json_file}")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ë¬¸ì„œ ì •ë³´ êµ¬ì„±
                doc_info = {
                    "doc_id": f"doc_{i:03d}",  # doc_001, doc_002, ...
                    "doc_name": data["file_name"],
                    "total_pages": data["total_pages"],
                    "pages": data["pages"]
                }
                
                self.documents.append(doc_info)
                print(f"      âœ“ {data['total_pages']}í˜ì´ì§€")
                
            except Exception as e:
                print(f"      âœ— ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nâœ… ì´ {len(self.documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n")
    
    def create_unified_structure(self) -> Dict:
        """í†µí•© ë°ì´í„° êµ¬ì¡° ìƒì„±"""
        print("ğŸ”§ í†µí•© ë°ì´í„° êµ¬ì¡° ìƒì„± ì¤‘...")
        
        unified_data = {
            "metadata": {
                "total_documents": len(self.documents),
                "documents": []
            },
            "text_blocks": []
        }
        
        block_counter = 1
        
        for doc in self.documents:
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            doc_meta = {
                "doc_id": doc["doc_id"],
                "doc_name": doc["doc_name"],
                "total_pages": doc["total_pages"]
            }
            unified_data["metadata"]["documents"].append(doc_meta)
            
            print(f"\n  ì²˜ë¦¬ ì¤‘: {doc['doc_name']}")
            
            # ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ text_blocksì— ì¶”ê°€
            for page in doc["pages"]:
                # ë¹ˆ í˜ì´ì§€ëŠ” ìŠ¤í‚µ
                if not page.get("content", "").strip():
                    continue
                
                text_block = {
                    "block_id": f"block_{block_counter:05d}",
                    "doc_id": doc["doc_id"],
                    "doc_name": doc["doc_name"],
                    "page": page["page_number"],
                    "text": page["content"]
                }
                
                unified_data["text_blocks"].append(text_block)
                block_counter += 1
            
            print(f"    âœ“ {len(doc['pages'])}í˜ì´ì§€ â†’ {block_counter - 1}ê°œ ë¸”ë¡")
        
        total_blocks = len(unified_data["text_blocks"])
        print(f"\nâœ… ì´ {total_blocks}ê°œ í…ìŠ¤íŠ¸ ë¸”ë¡ ìƒì„± ì™„ë£Œ")
        
        return unified_data
    
    def save_unified_data(self, output_path: str):
        """í†µí•© ë°ì´í„° ì €ì¥"""
        unified_data = self.create_unified_structure()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.dirname(output_path)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # JSON ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(unified_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ í†µí•© ë°ì´í„° ì €ì¥: {output_path}")
        print(f"  - ë¬¸ì„œ ìˆ˜: {unified_data['metadata']['total_documents']}ê°œ")
        print(f"  - í…ìŠ¤íŠ¸ ë¸”ë¡: {len(unified_data['text_blocks'])}ê°œ\n")

def main():
    print("="*80)
    print("ğŸ“„ ë¬¸ì„œ í†µí•© ì‹œì‘")
    print("="*80)
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    
    PROCESSED_DIR = os.path.join(project_root, "data", "processed")
    OUTPUT_PATH = os.path.join(project_root, "data", "processed", "construction_law_unified.json")
    
    # 1. ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ë§Œ
    if not os.path.exists(PROCESSED_DIR):
        print(f"\nâœ— ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {PROCESSED_DIR}")
        return
    
    # 2. ë°”ë¡œ í†µí•© ì‹¤í–‰
    try:
        merger = DocumentMerger(PROCESSED_DIR)
        merger.load_all_documents()
        
        if not merger.documents:
            print("âš  ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 3. ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ìë™ ë®ì–´ì“°ê¸°)
        merger.save_unified_data(OUTPUT_PATH)
        
        print("âœ… ë¬¸ì„œ í†µí•© ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()